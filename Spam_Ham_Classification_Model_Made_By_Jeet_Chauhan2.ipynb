{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNt1boZ+3+JxhMR0LB7qKJ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeetChauhan17/Spam-Ham-Classifier/blob/main/Spam_Ham_Classification_Model_Made_By_Jeet_Chauhan2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "443d9bcf"
      },
      "source": [
        "# Spam/Ham Classification - Made By Jeet S. Chauhan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a97f906a"
      },
      "source": [
        "This is a Spam/Ham Classification model made using Python with the help of many ML Liabraries like : SKlearn, Numpy, Pandas and NLTK. We have plotted the body length distribution at the end with the help of MatPlotLib Liabrary.\n",
        "\n",
        "This Project encompasses many topics in NLP. Topics such as Tokenization, Removing Stopwords, Stemming, Lemmenting, Vectorization, use of Sparse Matrix which ultimately help creating this project.\n",
        "\n",
        "This project works basically by reducing the contents of message into keywords with no punctuations and using that to train a model which can then classify new unseen messages into Spam or Ham.\n",
        "\n",
        "NLTK- Natural Language Toolkit- The NLTK is the most utilised package for handling natural language processing tasks. It is an open source library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLtGBBx5sQ7s"
      },
      "source": [
        "# Importing Libraries - NLTK, Pandas, Numpy :\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install nltk\n",
        "!pip install -U scikit-learn\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "3mWXVfxms2VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading The Dataset"
      ],
      "metadata": {
        "id": "54oNWZestb7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datas = pd.read_csv(\"SMSSpamCollection.tsv\", sep=\"\\t\", header=None)\n",
        "datas.columns =['label', 'body_text']\n",
        "datas.head()\n"
      ],
      "metadata": {
        "id": "EuQjK6cdtumm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the Contents using column names and index."
      ],
      "metadata": {
        "id": "-_tkZBtEvt5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datas['label'][0], datas['body_text'][0]"
      ],
      "metadata": {
        "id": "tIvUXlwdvMlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datas['body_text'][1]"
      ],
      "metadata": {
        "id": "llvg1_YXvkQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Shape Of Data :"
      ],
      "metadata": {
        "id": "7_0h0OHwwkO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The Dataset has {} Rows and {} Columns\".format(len(datas), len(datas.columns)))"
      ],
      "metadata": {
        "id": "dwQv417GwoQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Number Of Spam and Ham Data :"
      ],
      "metadata": {
        "id": "39k4hoaVw_l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"There is total {} Number of Spam Data and {} number of Ham data. Out of {} number of Data.\".format(len(datas[datas['label']==\"spam\"]), len(datas[datas['label']==\"ham\"]),len(datas)))"
      ],
      "metadata": {
        "id": "WZv5enCAxH46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of Missing Data :"
      ],
      "metadata": {
        "id": "optSOKWEz2gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"There are {} number of missing data.\".format(datas['label'].isnull().sum()))\n",
        "print(\"There are {} number of missing data.\".format(datas['body_text'].isnull().sum()))"
      ],
      "metadata": {
        "id": "_K31hJILz47c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing Data - Cleaning Up Data :\n"
      ],
      "metadata": {
        "id": "ByhZOFem0g2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Punctuation from body text :"
      ],
      "metadata": {
        "id": "HF8JwOHG0yUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def rem_punct(text):\n",
        "  nopunct_text = \"\".join([char for char in text\n",
        "                          if char not in string.punctuation])\n",
        "  return nopunct_text\n",
        "\n"
      ],
      "metadata": {
        "id": "eKUNWj3P1BuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datas['body_clean'] = datas['body_text'].apply(lambda x:rem_punct(x))\n",
        "datas.head()"
      ],
      "metadata": {
        "id": "Yd1SvB-n2D-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenization - Splitting sentences into tokens or keywords :"
      ],
      "metadata": {
        "id": "k8jVveKL2iYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "  tokens = re.split('\\W',text)\n",
        "  return tokens\n",
        "\n",
        "datas['tokenized_text'] = datas['body_clean'].apply(lambda x:tokenize(x.lower()))\n",
        "datas.head()"
      ],
      "metadata": {
        "id": "cVIu3BH82w_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Removing Stopwords - Removing unnecesaary words like the, but, etc."
      ],
      "metadata": {
        "id": "axqE-6A14XGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwrds = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def rem_stopwrds(tokenized_text):\n",
        "  text = [word for word in tokenized_text if word not in stopwrds]\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "4Mx-xKXj6Kgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datas['no_stop'] = datas['tokenized_text'].apply(lambda x:rem_stopwrds(x))\n",
        "datas.head()"
      ],
      "metadata": {
        "id": "ESS9KolG7A0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stemming - Reducing words with similar inflection or derived words to their stem or root :"
      ],
      "metadata": {
        "id": "lwos6loFAvpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps=nltk.PorterStemmer()\n",
        "\n",
        "def stemming(tokenized_text):\n",
        "    text=[ps.stem(word) for word in tokenized_text]\n",
        "    return text\n",
        "\n",
        "datas['stemmed_text']=datas['no_stop'].apply(lambda x:stemming(x))\n",
        "\n",
        "datas.head()"
      ],
      "metadata": {
        "id": "xWdTCcXsAdA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lemmatization - Grouping together inflected form of words so they can be analysied as a single term, the words lemma."
      ],
      "metadata": {
        "id": "g0ZXQYd6Bo7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wnl = nltk.WordNetLemmatizer()\n",
        "\n",
        "def lemmatizing(tokenized_text):\n",
        "  text = [wnl.lemmatize(word) for word in tokenized_text]\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "1ittTzYrCMyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datas['lemmatied_text'] = datas['no_stop'].apply(lambda x:lemmatizing(x))\n",
        "\n",
        "datas.head()"
      ],
      "metadata": {
        "id": "YWcOdM-3CyT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Vectorization - Process of encoding integers as feature vectors."
      ],
      "metadata": {
        "id": "mb67OlnYD2jy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Count Vectorization- Used to Create a document-term matrix that has entry of each cell which will be a count of the number of times that word occured in that document :\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lzyxlRG2EODW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def clean_text(text):\n",
        "  text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "  tokens = re.split('\\W',text)\n",
        "  text = [ps.stem(word) for word in tokens if word not in stopwrds]\n",
        "  return text\n",
        "\n",
        "count_vect = CountVectorizer(analyzer = clean_text)\n",
        "X_count = count_vect.fit_transform(datas['body_text'])\n",
        "\n",
        "print(X_count.shape)"
      ],
      "metadata": {
        "id": "OYlJlimLHI4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying Count Vectorization to small sample"
      ],
      "metadata": {
        "id": "5Y2vEE1NQ9FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_sample = datas[0:20]\n",
        "\n",
        "count_vect_sample = CountVectorizer(analyzer=clean_text)\n",
        "X_count_sample = count_vect_sample.fit_transform(data_sample['body_text'])\n",
        "\n",
        "print(X_count_sample.shape)"
      ],
      "metadata": {
        "id": "Bz9OEnZARRnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sparse Matrix - A Matrix of zeros and ones (Mostly zero). And to be efficient, it shows only non-zero Entries."
      ],
      "metadata": {
        "id": "I6uv5mhsSDDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_count_sample"
      ],
      "metadata": {
        "id": "A0NWvNg6SZ_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_count_df = pd.DataFrame(X_count_sample.toarray())\n",
        "X_count_df"
      ],
      "metadata": {
        "id": "nTVl6ylmSedA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "X_count_df.columns= count_vect_sample.get_feature_names_out()\n",
        "X_count_df"
      ],
      "metadata": {
        "id": "X7T-1qeDS4Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TF/IDF (Term Frequency, Inverse Document Frequency) - Creates Document amtrix where column represents Unirams and cells represent weighting which represents importance of word to the document."
      ],
      "metadata": {
        "id": "AyyeeW3iVCys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
        "X_tfidf = tfidf_vect.fit_transform(datas['body_text'])\n",
        "\n",
        "print(X_tfidf.shape)\n"
      ],
      "metadata": {
        "id": "NjYimbLZWKej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying TfidVectorizer to a small sample :\n",
        "\n"
      ],
      "metadata": {
        "id": "hg5qqersZEgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_sample = datas[0:20]\n",
        "\n",
        "tfidf_vect_sample = TfidfVectorizer(analyzer = clean_text)\n",
        "X_tfidf_sample = tfidf_vect_sample.fit_transform(data_sample['body_text'])\n",
        "\n",
        "print(X_tfidf_sample.shape)"
      ],
      "metadata": {
        "id": "5mXpM88iZ33-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering - Feature Creation :"
      ],
      "metadata": {
        "id": "CLnpWL75ajbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datas=pd.read_csv(\"SMSSpamCollection.tsv\",sep=\"\\t\",header=None)\n",
        "\n",
        "datas.columns=['label','body_text']\n",
        "\n",
        "datas.head()"
      ],
      "metadata": {
        "id": "yqL0XwkFapQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Creation - Text Message Length :\n"
      ],
      "metadata": {
        "id": "XlD1yhn_a09K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datas['body_len']=datas[\"body_text\"].apply(lambda x:len(x)-x.count(\" \"))\n",
        "\n",
        "datas.head()"
      ],
      "metadata": {
        "id": "pM2QIWetbHMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Creation - Percentage for Punctuation :"
      ],
      "metadata": {
        "id": "xiHFkgtEbVZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_punct(text):\n",
        "    count=sum([1 for char in text if char in string.punctuation])\n",
        "    return round(count/(len(text)-text.count(\" \")),3)*100\n",
        "\n",
        "datas['punct%']=datas['body_text'].apply(lambda x:count_punct(x))\n",
        "\n",
        "datas.head()"
      ],
      "metadata": {
        "id": "Ndghky6cbfv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plotting :"
      ],
      "metadata": {
        "id": "q3qzUcXDbpdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bins=np.linspace(0,200,40)\n",
        "\n",
        "plt.hist(datas['body_len'],bins)\n",
        "plt.title('Body Length Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2NvjSRIebuKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}